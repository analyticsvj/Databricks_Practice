ðŸ“˜ PET Control Objects DDL Bootstrap (pet_ctrl_objects_ddl.py)
ðŸ”¹ Purpose

This module ensures that PET control metadata tables are created safely and consistently in Delta Lake on Databricks.
It provides preflight/bootstrap utilities that:

Create the database at external storage locations (not DBFS/workspace bucket).

Create Delta tables idempotently (skip if exists).

Handle identity column compatibility (DBR 10.4 vs higher runtimes).

Validate storage locations (DESCRIBE DETAIL) to fail fast if a table is incorrectly located in DBFS.

Provide a re-create mechanism if tables are found on DBFS.

This guarantees that control/configuration metadata for PET ingestion always exists in the correct place, avoiding DeltaLog access issues due to permissions.

ðŸ”¹ Key Components

_ensure_database â€“ creates external DB via create_asset() helper if not present.

_create_if_missing â€“ creates Delta tables if not already present, retrying without identity if unsupported.

_describe_location â€“ validates table location is in external storage, not DBFS.

ensure_db_tables_and_verify_locations â€“ public entrypoint: ensures database + tables and verifies storage.

drop_and_recreate_tables â€“ recovery method if tables were initially created in DBFS.

ðŸ”¹ Flow (Step-by-Step)

Database Check & Create

Verify if DB exists (database_exists)

If not, call create_asset() â†’ ensures DB created under external storage.

Table Creation

For each control table definition:

Generate SQL DDL (Delta Lake + properties).

Try create with identity column.

If identity not supported â†’ retry without identity.

Location Verification

Run DESCRIBE DETAIL to get table location.

If DBFS/workspace bucket â†’ raise error.

Recovery Option

If error due to DBFS location â†’ run drop_and_recreate_tables() once.

ðŸ”¹ Flow Chart
  flowchart TD
    A[Start: Run pet_ctrl_objects_ddl] --> B{Database exists?}
    B -- No --> C[create_asset() -> External DB]
    B -- Yes --> D[Skip DB create]

    D --> E[Iterate PET Tables]
    C --> E

    E --> F{Table exists?}
    F -- Yes --> G[Skip creation]
    F -- No --> H[Generate DDL with identity]

    H --> I[Try CREATE TABLE]
    I -- Success --> J[Describe Location]
    I -- Fail + Identity cols --> K[Retry without identity]
    K --> J

    J --> L{Location = DBFS?}
    L -- Yes --> M[Raise error -> Use drop_and_recreate_tables()]
    L -- No --> N[Table OK]

    N --> O[Finish]
ðŸ”¹ Data Model
1. ctrl_dataset_config

Dataset-level control metadata

Column	Type	Notes
dataset_id	STRING (PK)	Unique dataset identifier
dataset_name	STRING	Human-readable name
source_database_name	STRING	Source DB
load_type	STRING	Full / Incremental
is_enabled	BOOLEAN	Dataset active flag
pet_dataset_id	STRING	Internal PET ID
incremental_timestamp_method	STRING	Method (HEADER / DATA TABLE)
incremental_timestamp_field	STRING	Timestamp col
incremental_header_to_tables_link_field	STRING	Link col
schedule	STRING	DAILY / WEEKLY / MONTHLY
week_days	STRING	Days of week (for weekly schedule)
dates_of_month	STRING	Dates (for monthly schedule)
dataset_owner	STRING	Owner
modified_date	TIMESTAMP	Last modified
dataset_modified_by_user	STRING	Who modified
last_run_time	TIMESTAMP	Last run
next_run_due_time	TIMESTAMP	Next due
last_process_datetime	MAP<STRING, TIMESTAMP>	Per-table timestamps
stage	STRING	Domain stage (Pass-1, etc.)
comments	STRING	Notes
2. ctrl_dataset_input_fields

Input field specification for datasets

Column	Type	Notes
field_id	BIGINT IDENTITY	Autogenerated
dataset_id	STRING	Links to dataset_config
database_name	STRING	Input DB
table_name	STRING	Input table
field_name	STRING	Input column
stage	STRING	Domain stage
comments	STRING	Notes
3. log_dataset_process

Dataset-level process log (per run)

Column	Type	Notes
process_load_id	STRING	Process run ID
dataset_id	STRING	Dataset
source_database_name	STRING	Source DB
target_file_name	STRING	File written
status	STRING	SUCCESS / FAILED
load_type	STRING	Full / Incremental
start_timestamp	TIMESTAMP	Run start
end_timestamp	TIMESTAMP	Run end
last_process_timestamp	TIMESTAMP	Last processed row
job_run_duration	STRING	Duration
error_message	STRING	Error if any
record_count	BIGINT	Rows processed
Partitioned by dataset_id.		
4. log_dataset_event

Event-level log within a process run

Column	Type	Notes
event_load_id	STRING	Event ID
process_load_id	STRING	Link to process log
dataset_id	STRING	Dataset
event_name	STRING	Event description
event_duration	INT	Duration (sec)
event_start_datetime	TIMESTAMP	Start
event_end_datetime	TIMESTAMP	End
event_status	STRING	SUCCESS / FAILED
event_message	STRING	Message
record_count	BIGINT	Records
Partitioned by dataset_id.		
ðŸ”¹ Data Model Diagram
erDiagram
    CTRL_DATASET_CONFIG ||--o{ CTRL_DATASET_INPUT_FIELDS : has
    CTRL_DATASET_CONFIG ||--o{ LOG_DATASET_PROCESS : triggers
    LOG_DATASET_PROCESS ||--o{ LOG_DATASET_EVENT : contains

    CTRL_DATASET_CONFIG {
        STRING dataset_id PK
        STRING dataset_name
        STRING source_database_name
        STRING load_type
        BOOLEAN is_enabled
        STRING schedule
    }
    CTRL_DATASET_INPUT_FIELDS {
        BIGINT field_id PK
        STRING dataset_id FK
        STRING database_name
        STRING table_name
        STRING field_name
    }
    LOG_DATASET_PROCESS {
        STRING process_load_id PK
        STRING dataset_id FK
        STRING status
        TIMESTAMP start_timestamp
        TIMESTAMP end_timestamp
    }
    LOG_DATASET_EVENT {
        STRING event_load_id PK
        STRING process_load_id FK
        STRING event_name
        STRING event_status
    }

