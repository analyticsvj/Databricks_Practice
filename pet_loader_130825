# -*- coding: utf-8 -*-
# PET configuration loader (Option C - memory-only Excel path)

from __future__ import annotations

import io
import os
import re
import logging
from datetime import datetime
from typing import Dict, Iterable, List, Optional, Tuple, Any, Union

import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql import Row, DataFrame
from pyspark.sql.functions import col
from pyspark.sql.types import (
    StructType,
    StructField,
    StringType,
    BooleanType,
    TimestampType,
    MapType,
)

# ------------------------------------------------------------------------------
# Logging
# ------------------------------------------------------------------------------
logger = logging.getLogger("PETFileLoader")
if not logger.handlers:
    logging.basicConfig(level=logging.INFO)


# ------------------------------------------------------------------------------
# Option C helpers: read Excel to memory (bytes -> pandas.ExcelFile)
# ------------------------------------------------------------------------------
def _split_s3_uri(s3_uri: str) -> Tuple[str, str]:
    if not s3_uri.lower().startswith("s3://"):
        raise ValueError(f"Expected s3:// URI, got: {s3_uri}")
    path = s3_uri[5:]
    first = path.find("/")
    if first < 0:
        raise ValueError(f"Bad S3 URI (missing key): {s3_uri}")
    bucket = path[:first]
    key = path[first + 1 :]
    return bucket, key


def fetch_xlsx_bytes(uri: str) -> bytes:
    """
    Return the Excel file contents as bytes.

    Supports:
      - s3://bucket/key.xlsx   (via boto3)
      - dbfs:/path/file.xlsx   (via open on /dbfs)
      - /dbfs/... or /local path
    """
    if uri.lower().startswith("s3://"):
        import boto3

        bucket, key = _split_s3_uri(uri)
        logger.info(f"PETFileLoader:Fetching bytes from s3://{bucket}/{key}")
        obj = boto3.client("s3").get_object(Bucket=bucket, Key=key)
        return obj["Body"].read()

    # DBFS convenience: allow dbfs:/ and /dbfs/
    if uri.lower().startswith("dbfs:/"):
        path = "/dbfs/" + uri[6:]
    else:
        path = uri

    with open(path, "rb") as f:
        data = f.read()
    logger.info(f"PETFileLoader:Fetched {len(data)} bytes from {uri}")
    return data


def open_excel(blob: bytes) -> pd.ExcelFile:
    """
    Create a pandas.ExcelFile from in-memory bytes using openpyxl.
    """
    bio = io.BytesIO(blob)
    return pd.ExcelFile(bio, engine="openpyxl")


# ------------------------------------------------------------------------------
# Metastore / table / column helpers
#  - canonical DB resolver fixes case/casing mismatches (DIDS vs dids)
#  - nested path resolution with manual walk + Spark analyzer fallback
# ------------------------------------------------------------------------------
def _resolve_db(spark: SparkSession, db: str) -> str:
    """
    Return canonical metastore schema name with exact casing.
    Supports 'schema' or 'catalog.schema'. If not found, return input.
    """
    parts = [p for p in db.split(".") if p]
    if not parts:
        return db

    # Databricks SHOW DATABASES lists schemas in current catalog
    try:
        dbs = [r.databaseName for r in spark.sql("SHOW DATABASES").collect()]
    except Exception:
        dbs = []

    if len(parts) == 1:
        schema = parts[0]
        return next((d for d in dbs if d.lower() == schema.lower()), schema)

    if len(parts) >= 2:
        catalog, schema = parts[0], parts[1]
        canon_schema = next((d for d in dbs if d.lower() == schema.lower()), schema)
        return f"{catalog}.{canon_schema}"


def _q(name: str) -> str:
    return f"`{name.replace('`', '``')}`"


def _qualify(spark: SparkSession, db: str, tbl: str) -> str:
    canon_db = _resolve_db(spark, db)
    parts = [p for p in canon_db.split(".") if p]
    if len(parts) == 1:
        return f"{_q(parts[0])}.{_q(tbl)}"
    if len(parts) == 2:
        return f"{_q(parts[0])}.{_q(parts[1])}.{_q(tbl)}"
    return ".".join([_q(p) for p in parts] + [_q(tbl)])


def _show_tables_in(spark: SparkSession, db: str, like: str):
    canon_db = _resolve_db(spark, db)
    parts = [p for p in canon_db.split(".") if p]
    like_ = like.replace("`", "``")
    if len(parts) == 1:
        return spark.sql(f"SHOW TABLES IN {_q(parts[0])} LIKE '{like_}'")
    elif len(parts) == 2:
        return spark.sql(f"SHOW TABLES IN {_q(parts[0])}.{_q(parts[1])} LIKE '{like_}'")
    return None


def _table_exists(spark: SparkSession, db: str, tbl: str) -> bool:
    try:
        df = _show_tables_in(spark, db, tbl)
        if df is not None:
            return df.limit(1).count() > 0
    except Exception:
        pass
    # Fallback: listTables in the schema part
    try:
        canon_db = _resolve_db(spark, db)
        schema = canon_db.split(".")[-1]
        return any(t.name.lower() == tbl.lower() for t in spark.catalog.listTables(schema))
    except Exception:
        return False


def _canonicalize_path(schema: StructType, path: str) -> Optional[str]:
    """
    Return the exact-cased dotted path found in schema (case-insensitive).
    E.g., 'meta.event_received_ts' -> 'META.EVENT_RECEIVED_TS'
    """
    parts = [p.strip() for p in path.split(".") if p.strip()]
    cur = schema
    resolved: List[str] = []
    for p in parts:
        if not isinstance(cur, StructType):
            return None
        fld = next((f for f in cur.fields if f.name.lower() == p.lower()), None)
        if fld is None:
            return None
        resolved.append(fld.name)
        cur = fld.dataType
    return ".".join(resolved)


def _quote_path_for_select(path: str) -> str:
    # `META`.`EVENT_RECEIVED_TS`
    return "`.`".join([p.replace("`", "``") for p in path.split(".") if p])



def check_table_and_column_exist(spark: SparkSession, db_name: str, table_name: str, column_name: str) -> bool:
    """
    Check if a table and column exist. Supports nested struct fields with dot notation.
    Example: check_table_and_column_exist(spark, "DIDS", "dids", "meta.event_received_ts")
    
    Uses multiple fallback approaches to handle permission issues:
    1. Try DESCRIBE TABLE (doesn't require reading Delta logs)
    2. Try spark.table().schema (requires Delta log access)
    3. Try SHOW COLUMNS (alternative approach)
    """
    logger.info(f"Checking table/column: {db_name}.{table_name}.{column_name}")
    
    try:
        # Check if table exists first
        tables_df = spark.sql(f"SHOW TABLES IN {db_name}")
        table_exists = tables_df.filter(col("tableName") == table_name).count() > 0
        logger.info(f"Table {db_name}.{table_name} exists: {table_exists}")
        
        if not table_exists:
            return False
        
        # Method 1: Try DESCRIBE TABLE (doesn't require Delta log access)
        try:
            logger.info(f"Attempting DESCRIBE TABLE for {db_name}.{table_name}")
            describe_df = spark.sql(f"DESCRIBE TABLE {db_name}.{table_name}")
            describe_rows = describe_df.collect()
            
            # Build a simple schema representation from DESCRIBE output
            available_columns = []
            for row in describe_rows:
                col_name = row['col_name']
                if col_name and not col_name.startswith('#'):  # Skip comments
                    available_columns.append(col_name.lower())
            
            logger.info(f"DESCRIBE TABLE found columns: {available_columns}")
            
            # Check for direct column match (case-insensitive)
            if column_name.lower() in available_columns:
                logger.info(f"Found direct column match for {column_name}")
                return True
            
            # For nested fields, we need more detailed schema info
            # If we have a dot notation, we might need the full schema
            if "." in column_name:
                logger.info(f"Column {column_name} contains dot notation, need full schema")
                # This requires falling back to schema reading
                raise Exception("Need full schema for nested field validation")
                
            logger.info(f"Column {column_name} not found in DESCRIBE TABLE output")
            return False
            
        except Exception as describe_error:
            logger.warning(f"DESCRIBE TABLE failed for {db_name}.{table_name}: {describe_error}")
            
            # Method 2: Try getting the schema (may fail with permission issues)
            try:
                logger.info(f"Attempting schema access for {db_name}.{table_name}")
                df_schema = spark.table(f"{db_name}.{table_name}").schema
                
                # Log schema structure for debugging
                schema_fields = [f.name for f in df_schema.fields]
                logger.info(f"Schema fields: {schema_fields}")
                
                # Check for direct column match first (case-insensitive)
                for field in df_schema.fields:
                    if field.name.lower() == column_name.lower():
                        logger.info(f"Found direct schema match for {column_name}")
                        return True
                
                # Check for struct field (e.g., meta.event_received_ts)
                if "." in column_name:
                    parts = column_name.split(".", 1)  # Split only on first dot
                    struct_name = parts[0].lower()
                    nested_field = parts[1].lower()
                    
                    logger.info(f"Looking for struct field: {struct_name} containing {nested_field}")
                    
                    # Find the struct field (case-insensitive)
                    for field in df_schema.fields:
                        if field.name.lower() == struct_name:
                            logger.info(f"Found struct field {field.name}, checking if it's a StructType")
                            # Check if it's a struct type and contains the nested field
                            if hasattr(field.dataType, 'fields'):  # StructType
                                nested_fields = [nf.name for nf in field.dataType.fields]
                                logger.info(f"Struct {field.name} contains fields: {nested_fields}")
                                found_nested = any(nested_f.name.lower() == nested_field for nested_f in field.dataType.fields)
                                if found_nested:
                                    logger.info(f"Found nested field {nested_field} in struct {field.name}")
                                    return True
                                else:
                                    logger.info(f"Nested field {nested_field} not found in struct {field.name}")
                            else:
                                logger.info(f"Field {field.name} is not a struct type: {type(field.dataType)}")
                            break
                    else:
                        logger.info(f"Struct field {struct_name} not found in schema")
                
                logger.info(f"Column {column_name} not found in schema")
                return False
                
            except Exception as schema_error:
                logger.warning(f"Schema access failed for {db_name}.{table_name}: {schema_error}")
                
                # Method 3: Try SHOW COLUMNS as last resort
                try:
                    logger.info(f"Attempting SHOW COLUMNS for {db_name}.{table_name}")
                    columns_df = spark.sql(f"SHOW COLUMNS IN {db_name}.{table_name}")
                    columns = [row['col_name'].lower() for row in columns_df.collect()]
                    
                    logger.info(f"SHOW COLUMNS found: {columns}")
                    
                    # For nested fields, this won't work well, but we can try basic matching
                    if column_name.lower() in columns:
                        logger.info(f"Found column {column_name} in SHOW COLUMNS")
                        return True
                    
                    # For nested fields like "meta.event_received_ts", check if "meta" exists
                    if "." in column_name:
                        struct_name = column_name.split(".", 1)[0].lower()
                        if struct_name in columns:
                            # We found the struct column, but can't verify the nested field
                            # without schema access. Return True and hope for the best.
                            logger.warning(f"Found struct column '{struct_name}' but cannot verify nested field due to permission issues")
                            return True
                    
                    logger.info(f"Column {column_name} not found in SHOW COLUMNS")
                    return False
                    
                except Exception as columns_error:
                    logger.error(f"All methods failed for {db_name}.{table_name}: DESCRIBE={describe_error}, SCHEMA={schema_error}, COLUMNS={columns_error}")
                    # If all methods fail, assume the column doesn't exist
                    return False
        
    except Exception as e:
        logger.exception(f"Table/Column check failed: {e}")
        # Instead of raising, return False to indicate the column wasn't found
        # This prevents the entire process from failing due to permission issues
        return False


# ------------------------------------------------------------------------------
# Validators
# ------------------------------------------------------------------------------
def validate_required_sheets(sheetnames: list) -> bool:
    required_sheets = {"Specification", "Input Schema", "DPS-CDP Params"}
    missing = required_sheets - set(sheetnames)
    if missing:
        raise ValueError(f"Missing required sheets: {missing}")
    return True


def validate_schedule_rules(param_data: dict) -> bool:
    schedule = param_data.get("Schedule")
    week_days = param_data.get("Week Days")
    dates_of_month = param_data.get("Dates Of Month")

    if schedule not in ("DAILY", "WEEKLY", "MONTHLY"):
        raise ValueError(f"Invalid schedule '{schedule}'. Must be DAILY, WEEKLY, or MONTHLY.")

    if schedule == "DAILY":
        if week_days or dates_of_month:
            raise ValueError("For DAILY schedule, 'Week Days' and 'Dates Of Month' must be empty.")
    elif schedule == "WEEKLY":
        if not week_days:
            raise ValueError("For WEEKLY schedule, 'Week Days' must not be empty.")
        if dates_of_month:
            raise ValueError("For WEEKLY schedule, 'Dates Of Month' must be empty.")
    elif schedule == "MONTHLY":
        if not dates_of_month:
            raise ValueError("For MONTHLY schedule, 'Dates Of Month' must not be empty.")
        if week_days:
            raise ValueError("For MONTHLY schedule, 'Week Days' must be empty.")

    logger.info(f"Schedule validation passed for '{schedule}'")
    return True


def validate_dps_cdp_params(spark: SparkSession, param_data: dict, db_name: str, expected_stage: str) -> bool:
    load_type = (param_data.get("Load Type") or "").upper()

    # Stage validation
    stage = (param_data.get("Stage") or "").upper()
    if not stage:
        raise ValueError("Stage is missing in DPS-CDP Params sheet.")
    if stage != expected_stage.upper():
        raise ValueError(f"Stage mismatch: expected '{expected_stage}', found '{stage}'")
    if stage not in {"DOMAIN-0-TRANSFORMATION", "DATA-PROVISIONING"}:
        raise ValueError("Stage must be 'DOMAIN-0-TRANSFORMATION' or 'DATA-PROVISIONING'")

    if load_type not in {"INCREMENTAL", "FULL"}:
        raise ValueError("Load Type must be 'INCREMENTAL' or 'FULL'")

    if load_type == "INCREMENTAL":
        if not param_data.get("Incremental Timestamp Field"):
            raise ValueError("Incremental Timestamp Field must not be empty for INCREMENTAL load")
        validate_schedule_rules(param_data)
    else:
        # FULL should not have incremental/schedule params
        if (
            param_data.get("Incremental Timestamp Field")
            or param_data.get("Incremental Header To Tables  Link Field")
            or param_data.get("Schedule")
            or param_data.get("Week Days")
            or param_data.get("Dates Of Month")
        ):
            raise ValueError("For FULL load, incremental fields and schedule parameters must be empty")

    if not param_data.get("Pet Dataset ID"):
        raise ValueError("Pet Dataset ID is required")

    method = (param_data.get("Incremental Timestamp Method") or "").upper()
    if method not in {"TIMESTAMP_HEADER_TABLE", "TIMESTAMP_DATA_TABLE"}:
        raise ValueError("Incremental Timestamp Method must be 'TIMESTAMP_HEADER_TABLE' or 'TIMESTAMP_DATA_TABLE'")

    if method == "TIMESTAMP_HEADER_TABLE":
        value = param_data.get("Incremental Header To Tables  Link Field")
        if not value or "." not in value:
            raise ValueError("Incremental Header To Tables Link Field must be in '<table>.<field>' format")
        table, field = value.split(".", 1)
        if not check_table_and_column_exist(spark, db_name, table, field):
            raise ValueError(f"Table or field not found: {db_name}.{table}.{field}")

    return True


def check_database_exists(spark: SparkSession, db_name: str) -> bool:
    canon = _resolve_db(spark, db_name)
    return canon.lower() in [row.databaseName.lower() for row in spark.sql("SHOW DATABASES").collect()]


def validate_incremental_timestamp_field(spark: SparkSession, db_name: str, field_spec: Optional[str]) -> None:
    if not field_spec or "." not in field_spec.strip():
        raise ValueError(
            "Incremental Timestamp Field must include the table, e.g. 'table.col' or 'table.struct.inner'."
        )
    table, column_path = field_spec.strip().split(".", 1)
    table = table.strip()
    column_path = column_path.strip()

    ok = check_table_and_column_exist(spark, db_name, table, column_path)
    if not ok:
        raise ValueError(f"Not found → {db_name}.{table}.{column_path}")


def get_last_process_map(
    spark: SparkSession, dataset_id: str, start_process_date: Optional[str], input_fields_df: pd.DataFrame
) -> dict:
    try:
        existing_map = {}
        existing_df = spark.sql(
            f"SELECT last_process_datetime FROM pet.ctrl_dataset_config WHERE dataset_id = '{dataset_id}'"
        )
        if existing_df.count() > 0:
            existing_map = existing_df.collect()[0].asDict().get("last_process_datetime") or {}

        tables = input_fields_df["Table Name"].dropna().unique()
        merged_map = dict(existing_map)

        if start_process_date:
            try:
                init_date = datetime.strptime(start_process_date.strip(), "%Y-%m-%d %H:%M:%S")
            except ValueError:
                init_date = datetime.strptime(start_process_date.strip(), "%Y-%m-%d")

            for table in tables:
                if table not in merged_map:
                    merged_map[table] = init_date
        else:
            if not existing_map:
                raise ValueError(f"'Start Process From Date' is required for first-time run of dataset {dataset_id}")

        return merged_map
    except Exception as e:
        logger.exception(f"Failed to compute last_process_map for dataset {dataset_id}: {e}")
        raise


def validate_external_id(spec_data: dict, dataset_id: str) -> None:
    if (spec_data.get("ExternalID") or "").upper() != dataset_id.upper():
        raise ValueError("Dataset ID mismatch between param and file")


def validate_dataset_name(spec_data: dict, dataset_id: str) -> None:
    if (spec_data.get("Dataset Name") or "").upper() != dataset_id.upper():
        raise ValueError("Dataset Name mismatch between param and file")


def validate_file_format(source_file_path: str) -> bool:
    if not source_file_path.lower().endswith((".xlsx", ".xls")):
        raise ValueError(f"Invalid file format. Expected Excel file (.xlsx or .xls), got: {source_file_path}")
    return True


def validate_dataset_enabled(param_data: dict, dataset_id: str) -> None:
    val = (param_data.get("Is Enabled") or "TRUE").strip().upper()
    if val == "FALSE":
        raise Exception(f"Dataset {dataset_id} is disabled (Is Enabled = FALSE). Skipping load.")


def validate_database_name(spark: SparkSession, param_data: dict) -> str:
    db_name = param_data.get("Database Name")
    if not db_name:
        raise ValueError("Database Name missing")
    if not check_database_exists(spark, db_name):
        raise ValueError(f"Database '{db_name}' not found")
    return db_name


def validate_input_fields(spark: SparkSession, input_fields_df: pd.DataFrame, default_db_name: str) -> pd.DataFrame:
    """
    Validates the 'Input Schema' sheet which contains:
      - 'Database Name'
      - 'Table Name'
      - 'Column Name'
    For each row:
      * if Database Name is blank, falls back to default_db_name (from DPS-CDP Params)
      * validates database/table/column (supports struct access via dot notation)
    Returns a cleaned DataFrame with exactly those three columns.
    """
    if input_fields_df is None or input_fields_df.empty:
        raise ValueError("Input Schema sheet is empty.")

    # Only keep first 3 columns; drop fully empty rows
    df = input_fields_df.iloc[:, :3].copy().dropna(how="all")
    df.columns = [str(c).strip() for c in df.columns]

    required_cols = {"Database Name", "Table Name", "Column Name"}
    missing = required_cols - set(df.columns)
    if missing:
        raise ValueError(f"Input Schema is missing required columns: {missing}")

    # Normalize cells
    df["Database Name"] = df["Database Name"].apply(lambda x: str(x).strip() if pd.notnull(x) else "")
    df["Table Name"] = df["Table Name"].apply(lambda x: str(x).strip() if pd.notnull(x) else "")
    df["Column Name"] = df["Column Name"].apply(lambda x: str(x).strip() if pd.notnull(x) else "")

    cleaned_rows = []
    for idx, row in df.iterrows():
        # Fallback to DPS-CDP 'Database Name' if blank
        db_name = row["Database Name"] or (default_db_name or "")
        table = row["Table Name"]
        column = row["Column Name"]

        row_no = idx + 2  # header is row 1 in Excel
        if not db_name:
            raise ValueError(f"[Input Schema] Row {row_no}: 'Database Name' is empty and no default provided.")
        if not table:
            raise ValueError(f"[Input Schema] Row {row_no}: 'Table Name' is empty.")
        if not column:
            raise ValueError(f"[Input Schema] Row {row_no}: 'Column Name' is empty.")

        if not check_database_exists(spark, db_name):
            raise ValueError(f"[Input Schema] Row {row_no}: Database '{db_name}' not found.")

        if not check_table_and_column_exist(spark, db_name, table, column):
            raise ValueError(f"[Input Schema] Row {row_no}: Not found → {db_name}.{table}.{column}")

        cleaned_rows.append({"Database Name": db_name, "Table Name": table, "Column Name": column})

    return pd.DataFrame(cleaned_rows, columns=["Database Name", "Table Name", "Column Name"])


# ------------------------------------------------------------------------------
# Writers
# ------------------------------------------------------------------------------
def write_config(
    spark: SparkSession,
    dataset_id: str,
    spec_data: dict,
    param_data: dict,
    db_name: str,
    last_process_map: dict,
    overwrite: bool,
) -> bool:
    """
    Write to pet.ctrl_dataset_config; merge last_process_datetime if dataset_id+stage exists.
    """
    try:
        stage = param_data.get("Stage")

        # 1) existing row (dataset_id + stage)
        existing_df = spark.sql(
            f"""
            SELECT * FROM pet.ctrl_dataset_config
            WHERE dataset_id = '{dataset_id}'
              AND stage = '{stage}'
            """
        )
        merged_last_process_map = {}
        if existing_df.count() > 0:
            existing_record = existing_df.collect()[0].asDict()
            existing_last = existing_record.get("last_process_datetime") or {}
            merged_last_process_map.update(existing_last)
            merged_last_process_map.update(last_process_map)
        else:
            merged_last_process_map = last_process_map

        # 2) row payload
        config_row = {
            "dataset_id": dataset_id,
            "dataset_name": spec_data.get("Dataset Name"),
            "source_database_name": db_name,
            "load_type": param_data.get("Load Type"),
            "is_enabled": True,
            "pet_dataset_id": param_data.get("Pet Dataset ID"),
            "incremental_timestamp_method": param_data.get("Incremental Timestamp Method"),
            "incremental_timestamp_field": param_data.get("Incremental Timestamp Field"),
            "incremental_header_to_tables_link_field": param_data.get(
                "Incremental Header To Tables  Link Field"
            ),
            "schedule": param_data.get("Schedule"),
            "week_days": param_data.get("Week Days"),
            "dates_of_month": param_data.get("Dates Of Month"),
            "dataset_owner": param_data.get("Dataset Owner"),
            "modified_date": datetime.now(),
            "dataset_modified_by_user": os.getenv("USER", "system"),
            "last_run_time": None,
            "next_run_due_time": None,
            "last_process_datetime": merged_last_process_map,
            "stage": stage,
            "comments": param_data.get("Comments"),
        }

        config_schema = StructType(
            [
                StructField("dataset_id", StringType(), True),
                StructField("dataset_name", StringType(), True),
                StructField("source_database_name", StringType(), True),
                StructField("load_type", StringType(), True),
                StructField("is_enabled", BooleanType(), True),
                StructField("pet_dataset_id", StringType(), True),
                StructField("incremental_timestamp_method", StringType(), True),
                StructField("incremental_timestamp_field", StringType(), True),
                StructField("incremental_header_to_tables_link_field", StringType(), True),
                StructField("schedule", StringType(), True),
                StructField("week_days", StringType(), True),
                StructField("dates_of_month", StringType(), True),
                StructField("dataset_owner", StringType(), True),
                StructField("modified_date", TimestampType(), True),
                StructField("dataset_modified_by_user", StringType(), True),
                StructField("last_run_time", TimestampType(), True),
                StructField("next_run_due_time", TimestampType(), True),
                StructField("last_process_datetime", MapType(StringType(), TimestampType()), True),
                StructField("stage", StringType(), True),
                StructField("comments", StringType(), True),
            ]
        )

        config_df = spark.createDataFrame([Row(**config_row)], schema=config_schema)
        config_df.createOrReplaceTempView("tmp_config")

        # 3) delete + insert only the dataset_id + stage
        spark.sql(
            f"""
            DELETE FROM pet.ctrl_dataset_config
            WHERE dataset_id = '{dataset_id}'
              AND stage = '{stage}'
            """
        )
        spark.sql("INSERT INTO pet.ctrl_dataset_config SELECT * FROM tmp_config")

        logger.info(f"Configuration written for dataset {dataset_id} (stage={stage}).")
        return True

    except Exception as e:
        logger.exception(f"Failed to write to pet.ctrl_dataset_config: {e}")
        raise


def write_input_fields_table(spark: SparkSession, df: pd.DataFrame, dataset_id: str, stage: str) -> None:
    """
    Write Input Schema rows to pet.ctrl_dataset_input_fields:
      (dataset_id, database_name, table_name, field_name, comments, stage)
    """
    try:
        data = df.copy()
        data["dataset_id"] = dataset_id
        data.rename(
            columns={
                "Database Name": "database_name",
                "Table Name": "table_name",
                "Column Name": "field_name",
            },
            inplace=True,
        )
        data["comments"] = None
        data["stage"] = stage

        spark_df = spark.createDataFrame(
            data[["dataset_id", "database_name", "table_name", "field_name", "comments", "stage"]]
        )
        spark_df.createOrReplaceTempView("tmp_input_fields")

        # delete + insert only this dataset+stage
        spark.sql(
            f"""
            DELETE FROM pet.ctrl_dataset_input_fields
            WHERE dataset_id = '{dataset_id}'
              AND stage = '{stage}'
            """
        )

        spark.sql(
            """
            INSERT INTO pet.ctrl_dataset_input_fields
              (dataset_id, database_name, table_name, field_name, comments, stage)
            SELECT dataset_id, database_name, table_name, field_name, comments, stage
            FROM tmp_input_fields
            """
        )

        logger.info(f"Input Schema written for dataset {dataset_id} (stage={stage}).")
    except Exception as e:
        logger.exception(f"Failed to write to pet.ctrl_dataset_input_fields: {e}")
        raise


# ------------------------------------------------------------------------------
# Incremental Timestamp normalizer (accepts unqualified form)
# ------------------------------------------------------------------------------
def validate_and_normalize_incremental_timestamp_field(
    spark: SparkSession,
    db_name: str,
    field_spec: Optional[str],
    candidate_tables: Optional[Iterable[str]] = None,
) -> str:
    """
    Accepts either:
      - 'table.col' or 'table.struct.inner'
      - 'struct.inner' (no table) -> search across candidate_tables from Input Schema
    Returns 'table.path' on success.
    """
    if not field_spec or not str(field_spec).strip():
        raise ValueError("Incremental Timestamp Field is missing")

    fs = str(field_spec).strip()
    parts = [p.strip() for p in fs.split(".") if p.strip()]

    # Case A: appears table-qualified & table exists
    if len(parts) >= 2 and _table_exists(spark, db_name, parts[0]):
        table = parts[0]
        col_path = ".".join(parts[1:])
        if check_table_and_column_exist(spark, db_name, table, col_path):
            return f"{table}.{col_path}"
        raise ValueError(f"Not found → {db_name}.{table}.{col_path}")

    # Case B: unqualified; search candidate tables
    if not candidate_tables:
        raise ValueError(
            f"Incremental Timestamp Field '{fs}' is not qualified with a table name. "
            f"Please specify '<table>.{fs}'."
        )

    matches: List[str] = []
        path_only = ".".join(parts)
    scanned_tables = []
    for t in candidate_tables:
        t_clean = str(t).strip()
        if not t_clean:
            continue
        scanned_tables.append(t_clean)
        try:
            if check_table_and_column_exist(spark, db_name, t_clean, path_only):
                matches.append(t_clean)
        except Exception:
            # ignore table-specific delta/permission issues during discovery
            continue

    if len(matches) == 1:
        return f"{matches[0]}.{path_only}"
    if len(matches) > 1:
        raise ValueError(
            f"Ambiguous Incremental Timestamp Field '{fs}' found in tables {matches}. "
            f"Please qualify it as '<table>.{path_only}'."
        )

    hint = f" Scanned tables: {scanned_tables}" if scanned_tables else ""
    raise ValueError(f"Not found → {db_name}.{path_only}{hint}")


# ------------------------------------------------------------------------------
# Loader class
# ------------------------------------------------------------------------------
class PETFileLoader:
    def __init__(self, spark: SparkSession):
        self.spark = spark

    def LoadConfigurationsForDataset(
        self,
        dataset_id: str,
        source_file_path: str,
        stage: str,
        overwrite: bool = False,
    ) -> dict:
        results = {
            "dataset_id": dataset_id,
            "status": "FAILED",
            "errors": [],
            "config_written": False,
            "input_fields_written": False,
        }

        try:
            # 1) File sanity
            validate_file_format(source_file_path)

            # 2) Fetch into memory (Option C)
            blob = fetch_xlsx_bytes(source_file_path)
            xls = open_excel(blob)

            # 3) Required sheets
            validate_required_sheets(xls.sheet_names)

            # 4) Spec (simple placeholder; wire in a full parser if needed)
            spec_data = {"Dataset Name": dataset_id}

            # 5) DPS-CDP Params -> dict
            param_data: Dict[str, Optional[str]] = {}
            param_df = pd.read_excel(
                xls, sheet_name="DPS-CDP Params", dtype=str, keep_default_na=False, engine="openpyxl"
            )
            for k, v in zip(param_df.iloc[:, 0], param_df.iloc[:, 1]):
                if k is None:
                    continue
                key = str(k).strip()
                if not key:
                    continue
                val = None if v is None else str(v).strip()
                if val == "":
                    val = None
                param_data[key] = val

            # 6) Enabled?
            validate_dataset_enabled(param_data, dataset_id)

            # 7) Database name
            db_name = validate_database_name(self.spark, param_data)

            # 8) DPS-CDP params (stage + load-type/method checks)
            validate_dps_cdp_params(self.spark, param_data, db_name, stage)

            # 9) Read Input Schema (raw) to get candidate tables for TS inference
            raw_input_df = pd.read_excel(
                xls, sheet_name="Input Schema", dtype=str, keep_default_na=False, engine="openpyxl"
            )
            if "Table Name" in raw_input_df.columns:
                candidate_tables = (
                    raw_input_df["Table Name"]
                    .dropna()
                    .astype(str)
                    .str.strip()
                    .replace("", pd.NA)
                    .dropna()
                    .unique()
                    .tolist()
                )
            else:
                candidate_tables = (
                    raw_input_df.iloc[:, 1]
                    .dropna()
                    .astype(str)
                    .str.strip()
                    .replace("", pd.NA)
                    .dropna()
                    .unique()
                    .tolist()
                )

            # 10) Normalize/validate Incremental Timestamp Field
            normalized_ts = validate_and_normalize_incremental_timestamp_field(
                self.spark, db_name, param_data.get("Incremental Timestamp Field"), candidate_tables=candidate_tables
            )
            param_data["Incremental Timestamp Field"] = normalized_ts

            # 11) Validate Input Schema fully (per-row DB/table/column)
            cleaned_input_fields_df = validate_input_fields(self.spark, raw_input_df, db_name)

            # 12) last_process_map
            last_process_map = get_last_process_map(
                self.spark, dataset_id, param_data.get("Start Process From Date"), cleaned_input_fields_df
            )

            # 13) Write config
            config_written = write_config(
                self.spark, dataset_id, spec_data, param_data, db_name, last_process_map, overwrite
            )
            results["config_written"] = bool(config_written)

            # 14) Write Input Schema
            stage_value = param_data.get("Stage") or stage
            write_input_fields_table(self.spark, cleaned_input_fields_df, dataset_id, stage_value)
            results["input_fields_written"] = True

            results["status"] = "SUCCESS"
            return results

        except Exception as e:
            logger.exception(f"Error during config load: {e}")
            results["errors"].append(str(e))
            return results


# ------------------------------------------------------------------------------
# Databricks widgets entrypoint (optional for jobs)
# ------------------------------------------------------------------------------
if "dbutils" in locals():
    dbutils.widgets.text("dataset_id", "")
    dbutils.widgets.text("source_file_path", "")
    dbutils.widgets.dropdown("stage", "Data-Provisioning", ["Data-Provisioning", "Domain-0-Transformation"])
    dbutils.widgets.dropdown("overwrite", "False", ["True", "False"])

    spark = SparkSession.builder.getOrCreate()
    dataset_id = dbutils.widgets.get("dataset_id")
    source_file_path = dbutils.widgets.get("source_file_path")
    stage = dbutils.widgets.get("stage")
    overwrite = dbutils.widgets.get("overwrite") == "True"

    loader = PETFileLoader(spark)
    result = loader.LoadConfigurationsForDataset(dataset_id, source_file_path, stage, overwrite)

    print(result)
    dbutils.notebook.exit(result)
